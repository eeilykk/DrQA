{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03077fc3-8cc7-49b3-b1dc-05eab0730ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pathlib\n",
    "# temp = pathlib.PosixPath\n",
    "# pathlib.PosixPath = pathlib.WindowsPath\n",
    "# import drqa.tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "822c3dc5-f6bb-4bf7-9ebc-d4a07c32db53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this helps to solve error of notimplemented \n",
    "import pathlib\n",
    "temp = pathlib.PosixPath\n",
    "pathlib.PosixPath = pathlib.WindowsPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "657b053c-ce60-4652-bfca-67565609761d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import drqa.tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b09f0330-0169-4632-b5b9-817549309427",
   "metadata": {},
   "outputs": [],
   "source": [
    "drqa.tokenizers.set_default('corenlp_classpath', '/your/corenlp/classpath/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c9e181c2-e852-4ac7-8527-c6689bb570d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wexpect\n",
      "  Downloading wexpect-4.0.0.tar.gz (78 kB)\n",
      "     ---------------------------------------- 78.1/78.1 KB 4.2 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: pywin32>=220 in c:\\users\\kytan\\anaconda3\\lib\\site-packages (from wexpect) (227)\n",
      "Requirement already satisfied: psutil>=5.0.0 in c:\\users\\kytan\\anaconda3\\lib\\site-packages (from wexpect) (5.8.0)\n",
      "Building wheels for collected packages: wexpect\n",
      "  Building wheel for wexpect (setup.py): started\n",
      "  Building wheel for wexpect (setup.py): finished with status 'done'\n",
      "  Created wheel for wexpect: filename=wexpect-4.0.0-py3-none-any.whl size=46391 sha256=aa1a99216a6abe5410b8acd62287684950c98d142a62a0165420cde6a4392867\n",
      "  Stored in directory: c:\\users\\kytan\\appdata\\local\\pip\\cache\\wheels\\02\\1c\\3d\\691e034a4ba9db694a0dba7c3b32a838033ab186239c632a79\n",
      "Successfully built wexpect\n",
      "Installing collected packages: wexpect\n",
      "Successfully installed wexpect-4.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install wexpect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "edfeedd0-896e-46a6-a761-f786469d9072",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-9cd88353a8cb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwexpect\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDEFAULTS\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import json\n",
    "import wexpect\n",
    "\n",
    "from .tokenizer import Tokens, Tokenizer\n",
    "from . import DEFAULTS\n",
    "\n",
    "\n",
    "class CoreNLPTokenizer(Tokenizer):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            annotators: set that can include pos, lemma, and ner.\n",
    "            classpath: Path to the corenlp directory of jars\n",
    "            mem: Java heap memory\n",
    "        \"\"\"\n",
    "        self.classpath = (kwargs.get('classpath') or\n",
    "                          DEFAULTS['corenlp_classpath'])\n",
    "        self.annotators = copy.deepcopy(kwargs.get('annotators', set()))\n",
    "        self.mem = kwargs.get('mem', '2g')\n",
    "        self._launch()\n",
    "\n",
    "    def _launch(self):\n",
    "        \"\"\"Start the CoreNLP jar with pexpect.\"\"\"\n",
    "        annotators = ['tokenize', 'ssplit']\n",
    "        if 'ner' in self.annotators:\n",
    "            annotators.extend(['pos', 'lemma', 'ner'])\n",
    "        elif 'lemma' in self.annotators:\n",
    "            annotators.extend(['pos', 'lemma'])\n",
    "        elif 'pos' in self.annotators:\n",
    "            annotators.extend(['pos'])\n",
    "        annotators = ','.join(annotators)\n",
    "        options = ','.join(['untokenizable=noneDelete',\n",
    "                            'invertible=true'])\n",
    "        cmd = ['java', '-mx' + self.mem, '-cp', '\"%s\"' % self.classpath,\n",
    "               'edu.stanford.nlp.pipeline.StanfordCoreNLP', '-annotators',\n",
    "               annotators, '-tokenize.options', options,\n",
    "               '-outputFormat', 'json', '-prettyPrint', 'false']\n",
    "\n",
    "        # We use pexpect to keep the subprocess alive and feed it commands.\n",
    "        # Because we don't want to get hit by the max terminal buffer size,\n",
    "        # we turn off canonical input processing to have unlimited bytes.\n",
    "        self.corenlp = wexpect.spawn('/bin/bash', maxread=100000, timeout=60)\n",
    "        self.corenlp.setecho(False)\n",
    "        self.corenlp.sendline('stty -icanon')\n",
    "        self.corenlp.sendline(' '.join(cmd))\n",
    "        self.corenlp.delaybeforesend = 0\n",
    "        self.corenlp.delayafterread = 0\n",
    "        self.corenlp.expect_exact('NLP>', searchwindowsize=100)\n",
    "\n",
    "    @staticmethod\n",
    "    def _convert(token):\n",
    "        if token == '-LRB-':\n",
    "            return '('\n",
    "        if token == '-RRB-':\n",
    "            return ')'\n",
    "        if token == '-LSB-':\n",
    "            return '['\n",
    "        if token == '-RSB-':\n",
    "            return ']'\n",
    "        if token == '-LCB-':\n",
    "            return '{'\n",
    "        if token == '-RCB-':\n",
    "            return '}'\n",
    "        return token\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        # Since we're feeding text to the commandline, we're waiting on seeing\n",
    "        # the NLP> prompt. Hacky!\n",
    "        if 'NLP>' in text:\n",
    "            raise RuntimeError('Bad token (NLP>) in text!')\n",
    "\n",
    "        # Sending q will cause the process to quit -- manually override\n",
    "        if text.lower().strip() == 'q':\n",
    "            token = text.strip()\n",
    "            index = text.index(token)\n",
    "            data = [(token, text[index:], (index, index + 1), 'NN', 'q', 'O')]\n",
    "            return Tokens(data, self.annotators)\n",
    "\n",
    "        # Minor cleanup before tokenizing.\n",
    "        clean_text = text.replace('\\n', ' ')\n",
    "\n",
    "        self.corenlp.sendline(clean_text.encode('utf-8'))\n",
    "        self.corenlp.expect_exact('NLP>', searchwindowsize=100)\n",
    "\n",
    "        # Skip to start of output (may have been stderr logging messages)\n",
    "        output = self.corenlp.before\n",
    "        start = output.find(b'{\"sentences\":')\n",
    "        output = json.loads(output[start:].decode('utf-8'))\n",
    "\n",
    "        data = []\n",
    "        tokens = [t for s in output['sentences'] for t in s['tokens']]\n",
    "        for i in range(len(tokens)):\n",
    "            # Get whitespace\n",
    "            start_ws = tokens[i]['characterOffsetBegin']\n",
    "            if i + 1 < len(tokens):\n",
    "                end_ws = tokens[i + 1]['characterOffsetBegin']\n",
    "            else:\n",
    "                end_ws = tokens[i]['characterOffsetEnd']\n",
    "\n",
    "            data.append((\n",
    "                self._convert(tokens[i]['word']),\n",
    "                text[start_ws: end_ws],\n",
    "                (tokens[i]['characterOffsetBegin'],\n",
    "                 tokens[i]['characterOffsetEnd']),\n",
    "                tokens[i].get('pos', None),\n",
    "                tokens[i].get('lemma', None),\n",
    "                tokens[i].get('ner', None)\n",
    "            ))\n",
    "        return Tokens(data, self.annotators)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7b8e99a1-7f86-4ff6-821b-2d9a4f335cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from drqa.tokenizers import CoreNLPTokenizer\n",
    "# tok = CoreNLPTokenizer()\n",
    "# tok.tokenize('hello world').words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7415e30d-eb4c-4cdf-addd-35296e0930f9",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pexpect' has no attribute 'spawn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-3fe3dac3d192>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtok\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCoreNLPTokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtok\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'hello world'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive\\Desktop\\SMUACADS\\Y2S2\\TM\\github\\DrQA\\drqa\\tokenizers\\corenlp_tokenizer.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mannotators\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'annotators'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmem\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'mem'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'2g'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_launch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_launch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive\\Desktop\\SMUACADS\\Y2S2\\TM\\github\\DrQA\\drqa\\tokenizers\\corenlp_tokenizer.py\u001b[0m in \u001b[0;36m_launch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[1;31m# Because we don't want to get hit by the max terminal buffer size,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[1;31m# we turn off canonical input processing to have unlimited bytes.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m         \u001b[0mchild\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwexpect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspawn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cmd.exe'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorenlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mchild\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/bin/bash'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxread\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m60\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorenlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetecho\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'pexpect' has no attribute 'spawn'"
     ]
    }
   ],
   "source": [
    "tok = CoreNLPTokenizer()\n",
    "tok.tokenize('hello world').words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5192c857-c74b-470e-8711-fdf50a49a8eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
